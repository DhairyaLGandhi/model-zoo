{
 "cells": [
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Updating registry at `~/.julia/registries/General`\n",
      "  Updating git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h  Updating git-repo `https://github.com/JuliaGPU/CUDAdrv.jl.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h Resolving package versions...\n",
      "  Updating `~/modelzoo/model-zoo/other/bitstring-parity/Project.toml`\n",
      " [no changes]\n",
      "  Updating `~/modelzoo/model-zoo/other/bitstring-parity/Manifest.toml`\n",
      " [no changes]\n",
      "  Updating git-repo `https://github.com/JuliaGPU/CUDAnative.jl.git`\n",
      "\u001b[?25l    Fetching: [>                                        ]  0.0 %\r\u001b[2K\u001b[?25h Resolving package versions...\n",
      "  Updating `~/modelzoo/model-zoo/other/bitstring-parity/Project.toml`\n",
      "  [be33ccc6] ~ CUDAnative v0.10.1+ #master (https://github.com/JuliaGPU/CUDAnative.jl.git)\n",
      "  Updating `~/modelzoo/model-zoo/other/bitstring-parity/Manifest.toml`\n",
      "  [be33ccc6] ~ CUDAnative v0.10.1+ #master (https://github.com/JuliaGPU/CUDAnative.jl.git)\n",
      "  Building CUDAnative → `~/.julia/packages/CUDAnative/SvVtC/deps/build.log`\n",
      "  Updating git-repo `https://github.com/JuliaGPU/CuArrays.jl.git`\n",
      "\u001b[?25l    Fetching: [>                                        ]  0.0 %\r\u001b[2K\u001b[?25h Resolving package versions...\n",
      "  Updating `~/modelzoo/model-zoo/other/bitstring-parity/Project.toml`\n",
      " [no changes]\n",
      "  Updating `~/modelzoo/model-zoo/other/bitstring-parity/Manifest.toml`\n",
      " [no changes]\n",
      "  Updating git-repo `https://github.com/FluxML/Flux.jl.git`\n",
      "\u001b[?25l    Fetching: [>                                        ]  0.0 %\r\u001b[2K\u001b[?25h Resolving package versions...\n",
      "  Updating `~/modelzoo/model-zoo/other/bitstring-parity/Project.toml`\n",
      "  [587475ba] ~ Flux v0.6.10+ #master (https://github.com/FluxML/Flux.jl.git)\n",
      "  Updating `~/modelzoo/model-zoo/other/bitstring-parity/Manifest.toml`\n",
      "  [587475ba] ~ Flux v0.6.10+ #master (https://github.com/FluxML/Flux.jl.git)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg; Pkg.activate(\"/home/dhairyagandhi96/modelzoo/model-zoo/script/../other/bitstring-parity\"); Pkg.instantiate(); Pkg.add(PackageSpec(name=\"CUDAdrv\", rev = \"master\")); Pkg.add(PackageSpec(name=\"CUDAnative\", rev = \"master\")); Pkg.add(PackageSpec(name=\"CuArrays\", rev = \"master\")); Pkg.add(PackageSpec(name=\"Flux\", rev = \"master\"))\n",
    "\n",
    "include(\"data.jl\")\n",
    "using Flux, Statistics\n",
    "using Flux: onehot, onehotbatch, throttle, crossentropy, reset!, onecold\n",
    "using CuArrays\n",
    "\n",
    "const epochs = 10"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "training data of bit strings from length 2 to 10"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2000-element Array{Tuple{Flux.OneHotMatrix{CuArrays.CuArray{Flux.OneHotVector,1}},Flux.OneHotVector},1}:\n ([true false … false true; false true … true false], [false, true])\n ([true true … false false; false false … true true], [true, false])\n ([true true … true true; false false … false false], [true, false])\n ([false true; true false], [false, true])                          \n ([true true false; false false true], [false, true])               \n ([true; false], [true, false])                                     \n ([false true … false false; true false … true true], [true, false])\n ([false true … false true; true false … true false], [false, true])\n ([false true true false; true false false true], [true, false])    \n ([false true … true true; true false … false false], [false, true])\n ⋮                                                                  \n ([true; false], [true, false])                                     \n ([false true false true; true false true false], [true, false])    \n ([false true … false true; true false … true false], [false, true])\n ([true true … true false; false false … false true], [false, true])\n ([true false … false true; false true … true false], [false, true])\n ([true false … false true; false true … true false], [true, false])\n ([false true false; true false true], [true, false])               \n ([false; true], [false, true])                                     \n ([true true … false false; false false … true true], [false, true])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "train = gendata(2000, 1:10)\n",
    "train = gpu.(train)"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "validation data of bit strings of length 10"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...\n",
      "└ @ CUDAnative /home/dhairyagandhi96/.julia/packages/CUDAnative/SvVtC/src/compiler/rtlib.jl:156\n",
      "┌ Warning: ADAM(params) is deprecated; use ADAM(η::Float64) instead\n",
      "│   caller = top-level scope at none:0\n",
      "└ @ Core none:0\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18 [inlined]\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: calls to Base intrinsics might be GPU incompatible\n",
      "│   exception = (CUDAnative.MethodSubstitutionWarning(exp(x::T) where T<:Union{Float32, Float64} in Base.Math at special/exp.jl:75, exp(x::Float32) in CUDAnative at /home/dhairyagandhi96/.julia/packages/CUDAnative/SvVtC/src/device/libdevice.jl:90), Base.StackTraces.StackFrame[exp at exp.jl:75, mapreducedim_kernel_parallel at mapreduce.jl:29])\n",
      "└ @ CUDAnative /home/dhairyagandhi96/.julia/packages/CUDAnative/SvVtC/src/compiler/irgen.jl:101\n",
      "┌ Warning: calls to Base intrinsics might be GPU incompatible\n",
      "│   exception = (CUDAnative.MethodSubstitutionWarning(exp(x::T) where T<:Union{Float32, Float64} in Base.Math at special/exp.jl:75, exp(x::Float32) in CUDAnative at /home/dhairyagandhi96/.julia/packages/CUDAnative/SvVtC/src/device/libdevice.jl:90), Base.StackTraces.StackFrame[exp at exp.jl:75, mapreducedim_kernel_parallel at mapreduce.jl:29])\n",
      "└ @ CUDAnative /home/dhairyagandhi96/.julia/packages/CUDAnative/SvVtC/src/compiler/irgen.jl:101\n",
      "┌ Warning: calls to Base intrinsics might be GPU incompatible\n",
      "│   exception = (CUDAnative.MethodSubstitutionWarning(exp(x::T) where T<:Union{Float32, Float64} in Base.Math at special/exp.jl:75, exp(x::Float32) in CUDAnative at /home/dhairyagandhi96/.julia/packages/CUDAnative/SvVtC/src/device/libdevice.jl:90), Base.StackTraces.StackFrame[exp at exp.jl:75, mapreducedim_kernel_serial at mapreduce.jl:4])\n",
      "└ @ CUDAnative /home/dhairyagandhi96/.julia/packages/CUDAnative/SvVtC/src/compiler/irgen.jl:101\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18 [inlined]\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18\n",
      "└ @ Core ./string:18\n",
      "┌ Warning: train!(loss, data, opt) is deprecated; use train!(loss, params, data, opt) instead\n",
      "│   caller = top-level scope at string:18\n",
      "└ @ Core ./string:18\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "val = gendata(100, 10)\n",
    "val = gpu.(val)\n",
    "\n",
    "scanner = LSTM(length(alphabet), 20) |> gpu\n",
    "encoder = Dense(20, length(alphabet)) |> gpu\n",
    "\n",
    "function model(x)\n",
    "    state = scanner.(x.data)[end]\n",
    "    reset!(scanner)\n",
    "    softmax(encoder(state))\n",
    "end\n",
    "\n",
    "loss(x, y) = crossentropy(model(x), y)\n",
    "\n",
    "opt = ADAM(params(scanner, encoder))\n",
    "\n",
    "for i=1:epochs\n",
    "    Flux.train!(loss, train, opt)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "sanity test"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0]\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "tx = map(c -> onehotbatch(c, alphabet), [\n",
    "    [false, true], # 01 -> 1\n",
    "    [true, false], # 10 -> 1\n",
    "    [false, false], # 00 -> 0\n",
    "    [true, true]]) # 11 -> 0\n",
    "[onecold(model(x)) - 1 for x in tx] |> println"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Try running the model on strings of length 50.\n",
    "\n",
    "Even though the model has only been trained with\n",
    "much shorter strings, it has learned the\n",
    "parity function and will accurate on longer strings."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "t50 (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "function t50()\n",
    "    l = batch_loss(gendata(1000, 50))\n",
    "    println(l)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 5
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  },
  "kernelspec": {
   "name": "julia-1.0",
   "display_name": "Julia 1.0.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
